---
title: "Work Context in Copilot CLI & Testing Agent Skills"
date: 2026-01-24
tags:
  - daily-update
  - ai-pm-research
---

# Daily PM Research Update: 2026-01-24

## One-Line Summary

Microsoft connects organizational knowledge to coding agents via MCP, while Vercel's empirical tests challenge the "bash is all you need" narrative with real benchmarks.

---

## Items

### Microsoft - Work IQ Brings Organizational Context to GitHub Copilot CLI
**Source:** https://www.linkedin.com/posts/satyanadella_so-much-of-dev-work-happens-in-the-context-activity-7420485585376620544-vudJ
**Credibility:** High (first-party announcement, public preview available)

**What happened:** Microsoft launched Work IQ, an MCP server that connects organizational data (emails, meetings, documents, Teams messages, SharePoint) to GitHub Copilot CLI. Developers can now query institutional knowledge directly from the terminal.

**Key details:**
- Available as CLI tool (`npm install -g @microsoft/workiq`) or Copilot plugin
- Five data categories: emails, calendar, documents, Teams messages, people directory
- Example use case: compare specs stored in SharePoint against actual code implementations to find requirement drift

**Why it matters for PMs:**
This is Microsoft's play to compete with enterprise AI assistants like Glean. The MCP approach is notable—it's the same protocol Anthropic uses for Claude, meaning Microsoft is adopting an interoperability standard rather than building proprietary connectors. For PMs building AI tools, this signals MCP is becoming the de facto standard for context integration.

**Critical questions:**
- Requires *both* GitHub Copilot AND Microsoft 365 Copilot licenses—what's the real adoption barrier for teams?
- Security researchers have flagged prompt injection risks: malicious content in emails/documents could potentially exfiltrate data through the agent
- 67% of enterprise security teams express concerns about AI tools exposing sensitive information—how does this affect rollout strategy?

**Action you could take today:**
If you're building AI features that need organizational context, evaluate MCP as your integration protocol. Microsoft's adoption validates it as enterprise-ready.

---

### Vercel - Empirical Test: Is Bash Really All You Need for Agents?
**Source:** https://vercel.com/blog/testing-if-bash-is-all-you-need
**Credibility:** High (controlled benchmark with published methodology)

**What happened:** Vercel ran a rigorous empirical test of CEO Guillermo Rauch's viral claim that "bash is all you need" for AI agents. They tested three architectures on GitHub issue/PR querying tasks.

**The actual numbers:**

| Agent | Accuracy | Cost | Duration |
|-------|----------|------|----------|
| SQL | 100% | $0.51 | 45s |
| Bash | 52.7% | $3.34 | 401s |
| Filesystem | 63.0% | $3.89 | 126s |

SQL dominated: 7x fewer tokens, 6.5x lower cost, 9x faster.

**The nuanced finding:** A hybrid approach (SQL + bash verification) achieved 100% accuracy at ~2x the token cost of pure SQL. Bash caught edge cases SQL missed by double-checking results.

**Why it matters for PMs:**
This is rare in AI discourse—real numbers instead of vibes. Tool choice depends heavily on task structure: bash excels when data is in file hierarchies; SQL excels with relational data. The hybrid insight is actionable: consider verification layers rather than choosing a single approach.

**Critical questions:**
- This tested a *different* use case than the original claim (GitHub issues vs. structured analytics)—does the finding generalize?
- They found 5 errors in their own evaluation dataset—how confident should we be in any agent benchmark?

**Action you could take today:**
Before choosing agent architecture, profile your data structure. If relational, lean SQL. If file-based, lean bash. If accuracy is critical, budget for a hybrid verification layer.

---

### LangChain - Deep Agents Framework for Multi-Agent Systems
**Source:** https://www.blog.langchain.com/building-multi-agent-applications-with-deep-agents/
**Credibility:** Medium (open-source release, but no benchmarks provided)

**What happened:** LangChain released Deep Agents, an open-source framework (MIT licensed) for multi-agent systems built on LangGraph. Inspired by Claude Code, Deep Research, and Manus.

**Key technical details:**
- **Subagents**: Isolated worker agents with separate context windows—solve "context bloat" when tasks fill up the main agent's context
- **Skills**: Progressive capability disclosure via SKILL.md files (loads full instructions only on-demand)
- Built-in planning, filesystem, and shell tools with sandboxing
- Provider agnostic (Claude, OpenAI, Google)

**Why it matters for PMs:**
Context window limits are a real constraint for complex agent tasks. LangChain claims 67% fewer tokens using subagents in some scenarios, though no methodology was provided. The subagent pattern—spawning isolated workers that return only final results—is worth understanding as agent architectures mature.

**Critical questions:**
- No benchmarks comparing to CrewAI, AutoGen, or direct LangGraph—how do we evaluate this?
- LangChain's own advice: "Don't rush into multi-agent architectures. Start with a single agent." When is multi-agent actually warranted?
- Microsoft's AutoGen docs now recommend their newer "Agent Framework" instead—is AutoGen being deprecated?

**Action you could take today:**
If your agent tasks are hitting context limits or requiring diverse capabilities, experiment with spawning isolated subagents rather than one monolithic agent. But start simple first.

---

## Quick Hits

- **Vercel**: [Custom platform error pages now available](https://vercel.com/changelog/vercel-now-supports-customizing-platform-error-pages) (Jan 23)
- **Vercel**: [Faster deploys via improved function caching](https://vercel.com/changelog/faster-deploys-with-improved-function-caching) (Jan 23)
- **Google**: [Virtual Try-On reached GA on Vertex AI](https://cloud.google.com/vertex-ai/docs/release-notes) (Jan 23)

---

## This Week's Pattern

**Agent infrastructure is maturing fast.** In the past 4 days: Windows 365 for Agents (cloud infrastructure), GitHub Copilot SDK (embedding), Work IQ (organizational context), and Deep Agents (multi-agent orchestration). We're past "what can agents do?" and into "how do we deploy and scale them?"

---

## Reflection Prompt

Vercel's benchmark showed SQL at 100% accuracy vs. bash at 52.7%—but a hybrid approach also hit 100%.

**For your AI product:** Where might you be choosing "either-or" between approaches when a verification layer would give you the benefits of both? What's the cost of that extra layer worth to your users?

Complete your reflection in `/content/reflections/daily/2026-01-24.md`
