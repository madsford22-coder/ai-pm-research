---
title: "How PMs Are Actually Using AI Coding Tools & Agent Framework Necessity"
date: 2026-02-15
tags:
  - daily-update
  - ai-pm-research
---

# Daily PM Research Update: 2026-02-15

## One-Line Summary

Lenny's community survey reveals how PMs actually use AI coding tools for research and experimentation (not production deployment), while LangChain defends agent frameworks as essential infrastructure despite model improvements—showing the gap between AI-native development promise and current PM practice.

---

## Items

### Lenny Rachitsky - How PMs Are Actually Using AI Coding Tools: Community Survey Results
**Source:** https://www.lennysnewsletter.com/p/community-wisdom-how-pms-are-using
**Credibility:** High (community survey with concrete PM usage patterns)

**What happened:** Lenny published survey findings on how PMs actually use AI coding tools—revealing a gap between hype and practice. The headline: most PMs use AI for research and prototypes, not production features.

**Key usage patterns from survey respondents:**

**What PMs actually do with AI coding tools:**
- **Research and experimentation**: Build throwaway scripts to test hypotheses or analyze data
- **Quick prototypes**: Generate UI mockups or basic functionality to validate concepts
- **Data analysis**: Write Python scripts for cleaning data, running queries, or visualizing metrics
- **Documentation**: Generate API specs, README files, or technical documentation
- **Internal tools**: Build Slack bots, admin panels, or workflow automation

**What PMs don't do (yet):**
- **Ship production features**: Most survey respondents hand AI-generated code to engineers for review/rewrite
- **Complex business logic**: Avoid using AI for critical paths (payments, auth, core features)
- **Database migrations**: Reluctance to let AI modify production schemas
- **Performance-critical code**: Concerned about AI-generated code being inefficient

**The confidence gap:**
Survey respondents report using AI tools frequently but lack confidence in output quality. Pattern: "I use it daily for prototypes but wouldn't ship it to customers without engineer review."

**Why the gap exists:**
- **Quality uncertainty**: Can't reliably assess when AI code is good enough for production
- **Organizational barriers**: Engineering teams prefer their own code or require review of AI-generated code
- **Risk aversion**: Stakes are too high for customer-facing features
- **Skill limits**: PMs can describe features but can't debug when AI code breaks

**The actual productivity gain:**
AI tools accelerate exploration and validation, not deployment. The value: PMs test more ideas faster, not ship more features solo. This validates the "research and prototype" use case over "replace engineers."

**Why it matters for PMs:**
This grounds the vibe coding narrative (Feb 8, Feb 12) in reality. Despite tools like Claude Code and v0, most PMs use AI for experimentation—not autonomous feature shipping. For PM leaders evaluating AI tool adoption, the question is: do you want faster prototyping or actual feature deployment? Current practice optimizes for the former.

**Critical questions:**
- What would it take for PMs to trust AI-generated code for production? Better testing? Better AI? Different product architecture?
- Is the prototype-to-production handoff actually slower than PMs building prototypes in Figma or slides? Or does code accelerate validation?
- Which PM workflows benefit most from AI coding—those requiring data analysis, UI prototypes, or process automation?

**Action you could take today:**
If you manage PMs, survey your team: how are they actually using AI coding tools? Are they prototyping, researching, or shipping? If mostly prototyping, optimize tooling and training for that use case rather than expecting production-ready output.

---

### LangChain - Why Agent Frameworks Still Matter Despite Better Models
**Source:** https://blog.langchain.com/on-agent-frameworks-and-agent-observability/
**Credibility:** High (detailed technical argument with architecture analysis)

**What happened:** LangChain published a defense of agent frameworks—arguing that despite model reasoning improvements, frameworks remain essential for production systems. This was already covered in the Feb 14 update.

**Deduplication note:** This item appeared in full detail in yesterday's update (Feb 14). The core argument: models handle reasoning; frameworks handle orchestration, state, observability, and safety. Not repeating full analysis here.

**Why it's relevant today:**
LangChain's framework defense directly addresses the question raised by Lenny's PM survey findings: if PMs struggle to ship AI-generated code to production, is it because they lack frameworks (LangChain's claim) or because AI output quality isn't trusted regardless of infrastructure?

**The tension:**
- LangChain argues frameworks are production-critical
- Lenny's survey shows PMs avoid production deployment even with frameworks available
- Gap suggests trust problem, not infrastructure problem

**Action you could take today:**
If you're building agents but hesitant to deploy, diagnose whether your blocker is infrastructure (missing observability, state management) or trust (uncertain quality). If infrastructure, evaluate frameworks. If trust, focus on testing strategies and quality gates.

---

## Quick Hits

- **Vercel**: [Browserbase joins Agent Marketplace](https://vercel.com/changelog/browserbase-joins-the-vercel-agent-marketplace) - Headless browser automation for agents (Feb 12, listed Feb 14)
- **Microsoft**: [Healthcare agentic AI readiness research](https://www.microsoft.com/en-us/industry/blog/healthcare/2026/02/12/assessing-healthcares-agentic-ai-readiness-new-research-from-microsoft-and-the-health-management-academy/) - Survey shows adoption barriers in healthcare (Feb 12, listed Feb 14)
- **Google**: [GLM-5 available in Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/maas/zai-org/glm-5) - Experimental launch targeting complex systems engineering and agentic tasks (Feb 10, listed Feb 14)

---

## This Week's Pattern

**Gap between AI coding tool capability and PM adoption.** Lenny's survey reveals PMs use AI for research and prototypes—not production deployment—despite tools claiming to enable end-to-end feature shipping. LangChain argues frameworks are essential infrastructure, but PM hesitation suggests trust (not infrastructure) blocks adoption. The shift: AI-native development remains aspirational; current practice is AI-accelerated exploration.

---

## Reflection Prompt

Lenny's survey shows most PMs use AI coding tools for research and prototypes—but avoid shipping production features, even when tools claim to enable it.

**For your PM practice:** Are you using AI coding tools primarily for exploration (testing hypotheses, analyzing data) or for production work (shipping features customers see)? What would need to change—better AI, better frameworks, or different team dynamics—for you to trust AI-generated code in production?

Complete your reflection in `/content/reflections/daily/2026-02-15.md`