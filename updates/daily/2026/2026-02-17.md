---
title: "AI Analysis You Can Trust & Agent Frameworks in Production"
date: 2026-02-17
tags:
  - daily-update
  - ai-pm-research
---

# Daily PM Research Update: 2026-02-17

## One-Line Summary

Lenny reveals four critical practices that make AI-generated data analysis trustworthy, Simon ships tools that let agents demo their work visually, and Harrison validates that LangSmith adoption extends beyond LangChain users—showing AI moving from "generate outputs" to "prove reliability."

---

## Items

### Lenny Rachitsky - How to Do AI Analysis You Can Actually Trust
**Source:** https://www.lennysnewsletter.com/p/how-to-do-ai-analysis-you-can-actually
**Credibility:** High (detailed framework with specific validation techniques)

**What happened:** Lenny published a guide on making AI-generated data analysis reliable—addressing the trust gap when PMs delegate analysis to AI tools. The core insight: AI can analyze data fast, but you need systematic validation to trust the results.

**Key validation practices:**

**1. Sanity check the outputs with simple tests:**
- Compare AI results to manual spot checks on a small sample
- Verify totals and aggregations match expected ranges
- Check if trends align with known business patterns
- Example: AI reports 400% week-over-week growth → obviously wrong, manual check reveals data quality issue

**2. Ask AI to show its work:**
- Request the SQL/Python code that generated results
- Review logic for common mistakes (wrong joins, incorrect filters)
- Validate that the approach matches the question asked
- Example: "Show me the code you used to calculate retention" → reveals AI used 7-day window when 30-day was needed

**3. Cross-validate with multiple approaches:**
- Ask AI to solve the same problem differently (SQL vs. Python, different aggregation methods)
- Compare results—if they diverge, investigate why
- Use disagreement as a signal to dig deeper
- Example: Cohort retention via SQL vs. Pandas produces different numbers → uncovers timezone handling bug

**4. Establish known-good test cases:**
- Create reference analyses with verified answers
- Test AI tools against these benchmarks regularly
- Track accuracy over time as models change
- Example: Monthly revenue calculation with known correct answer → AI must match exactly or fail validation

**The trust threshold pattern:**
Lenny argues you can't blindly trust AI analysis, but you can build confidence through systematic validation. The trust builds over time as AI consistently passes validation checks.

**Why delegation still wins despite validation overhead:**
Even with validation, AI analysis is faster than manual work. The pattern: AI generates → human validates → iterate until correct. This beats manual analysis when validation is cheaper than generation.

**Why it matters for PMs:**
This addresses the adoption barrier documented throughout January-February: PMs use AI for prototypes but hesitate to rely on it for decisions. Lenny's validation framework provides a middle path—delegate analysis but verify systematically. For PMs evaluating AI analytics tools, the question becomes: does the tool support validation workflows (show code, cross-check methods, test cases)?

**Critical questions:**
- What's the validation overhead that makes AI slower than manual analysis?
- How do you decide which analyses require full validation versus spot checks?
- Does validation catch systematic biases in AI reasoning, or just obvious errors?
- When AI fails validation repeatedly, is that a signal to stop using it for that task?

**Action you could take today:**
Pick one analysis you regularly do (cohort retention, funnel conversion, user segmentation). Generate it with AI, then implement Lenny's validation: spot check the output, review the code, cross-validate with a different method. Measure how long validation takes versus generating from scratch manually.

---

### Simon Willison - Introducing Showboat and Rodney: Tools for Agents to Demo Their Work
**Source:** https://simonwillison.net/2026/Feb/10/showboat-and-rodney/
**Credibility:** High (detailed technical implementation with working demos)

**What happened:** Simon Willison shipped Showboat and Rodney—tools that let AI agents create visual demonstrations of their work. Showboat captures browser screenshots; Rodney records terminal sessions. Both integrate with LLM tool calling so agents can document what they built.

**Key architectural patterns:**

**Showboat (browser automation + screenshots):**
- Agents control headless browsers via tool calls
- Capture screenshots at each step of workflows
- Generate visual documentation showing "what I built"
- Example: Agent builds web form → captures screenshot → returns image URL as proof

**Rodney (terminal recording):**
- Records terminal sessions as agents execute commands
- Generates animated GIFs showing command output
- Documents CLI workflows visually
- Example: Agent runs database migration → records terminal → shows successful completion

**Why visual proof matters for agents:**
Text logs show what agents did but don't prove it worked. Screenshots and recordings provide visual evidence that outputs match intent. This shifts agent trust from "hope it worked" to "see it work."

**The demo-ability gap:**
Traditional automation runs invisibly—success is assumed if no errors. Agent workflows need demonstrability because users don't trust black-box execution. Visual proof closes that gap.

**Integration pattern:**
Both tools expose MCP (Model Context Protocol) interfaces so any agent framework (LangChain, Mastra, custom) can call them. The architecture: agent invokes tool → tool captures visual → returns URL → agent includes proof in response.

**Why it matters for PMs:**
This continues the observability pattern from LangSmith (Feb 10) and AEO tracking (Feb 10): agents need visibility into their actions. But Showboat/Rodney focus on end-user trust, not developer debugging. For PMs building agents that users review, the question becomes: can users see what the agent did without reviewing logs?

**Critical questions:**
- Storage costs for screenshots and recordings—who hosts, how long to retain?
- Privacy: when agents access user data or internal tools, can screenshots leak sensitive information?
- What's the latency overhead of capturing visuals mid-workflow?
- Does visual proof actually increase user trust, or just add cognitive load?

**Action you could take today:**
If you're building agents that generate UI, run commands, or manipulate data, prototype visual proof. Use existing tools (screenshot APIs, terminal recording) to capture one agent workflow end-to-end. Show the visual output to users and ask: does this make you trust the agent more?

---

### Harrison Chase / LangChain Community - LangSmith Adoption Beyond LangChain Users
**Source:** https://x.com/Hacubu/status/2023544370969055385
**Credibility:** Medium (community observation without detailed metrics)

**What happened:** A community member highlighted that many companies use LangSmith (LangChain's observability platform) without using the LangChain framework—validating observability as standalone product category.

**The separation pattern:**
Early assumption: LangSmith exists to support LangChain users. Reality: observability is valuable independent of framework choice. Companies use Mastra, custom agents, or other frameworks but still need LangSmith for tracing and debugging.

**Why this matters strategically:**
LangChain's Feb 13 post defending frameworks (covered Feb 14) argued frameworks remain essential despite model improvements. But LangSmith's cross-framework adoption suggests observability matters more than orchestration for some users. The implication: observability is the more defensible layer.

**What this signals about infrastructure:**
Agent infrastructure is fragmenting—multiple frameworks (LangChain, Mastra, custom), multiple model providers (OpenAI, Anthropic, Google), multiple execution environments (Vercel Sandboxes, E2B, Runloop). But observability consolidates because debugging is universal, not framework-specific.

**Why it matters for PMs:**
This informs build-versus-buy decisions for agent infrastructure. Observability platforms like LangSmith solve problems every agent needs (logging, tracing, error analysis). Frameworks solve orchestration, which varies by use case. For PMs evaluating agent tooling, the question is: which layers are commodity (observability) versus which require custom implementation (orchestration)?

**Critical questions:**
- What percentage of LangSmith users don't use LangChain? (No public data)
- Do framework-agnostic observability tools emerge as competitors?
- Does this pattern apply to other infrastructure layers (sandboxes, memory systems)?
- At what scale does observability overhead (cost, latency) outweigh debugging value?

**Action you could take today:**
If you're building agents without LangChain, evaluate LangSmith for observability. Test whether framework-agnostic tracing helps debug your agent workflows. This calibrates whether observability is valuable enough to justify vendor dependency.

---

## Quick Hits

- **Vercel**: [Improved streaming runtime logs exports](https://vercel.com/changelog/improved-streaming-runtime-logs-exports) - Better log handling for agent deployments (Feb 17)
- **Vercel**: [Qwen 3.5 Plus on AI Gateway](https://vercel.com/changelog/qwen-3-5-plus-is-on-ai-gateway) - Chinese foundation model now available (Feb 16)
- **Simon Willison**: [Chartroom and datasette-showboat tools](https://simonwillison.net/2026/Feb/17/chartroom-and-datasette-showboat/) - Extensions for agent-generated visualizations (Feb 17)
- **Geoffrey Litt**: [Prototyping UIs with vibe coding](https://x.com/geoffreylitt/status/2023589705246200289) - Reflection on UI generation tools (Feb 17)
- **Amjad Masad**: [Replit partners with RazorPay for Indian builders](https://x.com/amasad/status/2023646483275567385) - Payment integration expansion (Feb 17)

---

## This Week's Pattern

**Proving AI reliability through systematic validation.** Lenny's framework makes AI analysis trustworthy through sanity checks, code review, cross-validation, and test cases. Simon's Showboat/Rodney tools let agents prove their work visually. LangSmith's cross-framework adoption validates observability as universal need. The shift: AI moving from "generate fast" to "prove correct"—reliability requires infrastructure, not just capability.

---

## Reflection Prompt

Lenny argues AI analysis needs systematic validation—sanity checks, code review, cross-validation, and test cases—before you can trust results for decisions.

**For your AI analysis workflows:** Are you validating AI-generated insights, or trusting them by default? Pick one analysis you recently accepted from AI. Apply Lenny's framework: spot check the numbers, review the code, try a different method. Did validation catch errors? And how long did validation take versus generating the analysis manually?

Complete your reflection in `/content/reflections/daily/2026-02-17.md`