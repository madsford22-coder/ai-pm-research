---
title: "Claude Code for PM Workflows & Multi-Agent Onboarding at Scale"
date: 2026-01-19
tags:
  - daily-update
  - ai-pm-research
---

# Daily PM Research Update: 2026-01-19

## One-Line Summary

Teresa Torres demonstrates Claude Code as a PM productivity system with custom task management and research workflows, while Remote's case study reveals how to handle data too large for LLM context windows through hybrid reasoning-plus-code execution.

---

## Items

### Lenny's Podcast - Teresa Torres on Claude Code for PM Productivity
**Source:** https://www.lennysnewsletter.com/p/claude-code-for-product-managers
**Credibility:** High (detailed workflow demonstration from author of Continuous Discovery Habits)

**What happened:** Teresa Torres demonstrated how she uses Claude Code in the terminal—not browser-based AI—to build a personalized productivity system for research, writing, and task management.

**Key workflows demonstrated:**

**1. Custom Task Management System:**
Migrated from Trello to markdown-based tasks within Claude Code:
- Complete control over task structure
- Enhanced searchability across all tasks
- Task-related research embedded directly in task files

**2. Automated Research Collection:**
Daily digest workflow that:
- Searches academic papers via Google Scholar
- Automatically summarizes relevant research
- Delivers curated findings filtered by specific topics

**3. Writing Partnership:**
Uses Claude as a "pair programming" partner for prose:
- Peer review and refinement of content
- Research support during writing
- Maintains authentic voice while enhancing clarity

**The key insight—context management:**
Rather than one massive context file, Torres organizes information into "small, focused context files" tailored to specific projects. This keeps Claude effective and prevents context window degradation.

**Why it matters for PMs:**
This is a concrete example of moving beyond "ask Claude a question" to "Claude as an embedded workflow partner." The terminal-based approach matters because it integrates with existing file systems and tools. For PMs considering AI productivity tools, the pattern is: structure your work to give AI focused context, not everything at once.

**Critical questions:**
- How much setup time does a custom system require vs. productivity gained?
- What's the learning curve for non-technical PMs to use terminal-based AI?
- How do you maintain custom workflows as Claude Code evolves?

**Action you could take today:**
Pick one recurring PM workflow (weekly research, status updates, competitor analysis). Document it as a focused context file and try running it through Claude Code. Start small—one workflow, not your entire system.

---

### LangChain - Remote Case Study: Multi-Agent Customer Onboarding at Scale
**Source:** https://www.blog.langchain.com/customers-remote/
**Credibility:** High (production case study with specific architecture and results)

**What happened:** Remote needed to automate HR and payroll data migrations for thousands of customers. The core problem: large datasets exceed LLM context windows and cause hallucinations. Their solution: separate reasoning from execution.

**The architecture—hybrid reasoning + code execution:**

1. **LLM Reasoning Phase**: Uses LangChain's tool-calling to plan transformation steps
2. **Sandboxed Code Execution**: Python runs in WebAssembly to transform data
3. **Iterative Refinement**: Agent reviews outputs, generates additional code as needed
4. **Structured Output**: Final validated JSON is stored; large datasets never return to the model

**The key insight:** "Let the models reason, let code execute." Large data transformations happen in code—only the reasoning layer uses the LLM. This keeps context windows small and virtually eliminates hallucinations.

**Why LangGraph:**
- Node-and-edge model enables complex workflows with explicit success/failure/retry transitions
- Agent state is transparent and recoverable
- Better than raw LangChain for multi-step processes with branching logic

**Results:**
- Migrations now take hours instead of days
- Teams no longer write custom scripts per customer
- Deterministic code execution ensures reliability for sensitive payroll data

**Why it matters for PMs:**
If you're building AI features that touch large datasets, this pattern is essential: don't try to fit data into context windows. Use the LLM to reason about what to do, then execute in code. For PMs evaluating agent architectures, ask: does this handle datasets larger than the context window? If not, you'll hit a wall at scale.

**Critical questions:**
- WebAssembly sandboxing adds complexity—when is it worth the overhead?
- How do you debug agent failures in production when code execution is sandboxed?
- What's the latency impact of iterative refinement loops?

**Action you could take today:**
If your AI features process user-uploaded data, estimate your typical file sizes in tokens. If >50% of context capacity, you need an architecture that separates reasoning from data processing—Remote's pattern is a good reference.

---

## Quick Hits

- **Julie Zhuo**: [Playing Authentic Games](https://joulee.medium.com/playing-authentic-games-5fdfbe27242d) - Leadership reflection on authenticity in professional contexts (Jan 15)
- **Vercel**: [Recraft image models now on AI Gateway](https://vercel.com/changelog/recraft-image-models-now-on-ai-gateway) - Image generation model integration for creative AI tools (Jan 19)

---

## This Week's Pattern

**AI workflows becoming structured systems.** Teresa Torres builds markdown-based task management in Claude Code. Remote separates LLM reasoning from code execution. The shift: moving from ad-hoc AI conversations to repeatable, structured workflows that scale.

---

## Reflection Prompt

Teresa Torres organizes her context into "small, focused context files" rather than one massive document—keeping Claude effective and preventing degradation.

**For your AI productivity:** How much context do you typically provide AI tools? Have you experimented with focused, minimal context vs. comprehensive context? Which produces better outputs for your use cases?

Complete your reflection in `/content/reflections/daily/2026-01-19.md`
