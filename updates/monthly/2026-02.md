---
title: "February 2026 Research Summary"
date: 2026-02-01
tags:
  - monthly-summary
  - ai-pm-research
---

# February 2026 Research Summary

AI tools crossed a threshold this month: non-engineers are now getting paid to ship production code, and it's working. StrongDM hired a 3-person team with zero traditional coding skills to build customer-facing features using Claude and Cursor. Lenny built custom CLI tools in 2-4 hours that would've taken weeks before. This isn't demos or prototypes—it's production software with engineering review gates, deployed to real users.

The infrastructure catching up validates this shift isn't temporary. Perplexity's Council Mode orchestrates multiple specialized models instead of relying on one. Microsoft's enterprise survey reveals agent deployments require dedicated operations roles—trainers, monitors, coordinators—proving agents need management, not just deployment. The pattern across everything: AI products moving from "can it work?" to "how do we operate it at scale?"

## What Matters

- **"Vibe coding" emerged as a real job category.** StrongDM's AI team (former sales engineer, PM, designer—no coders) ships production features in 1-3 weeks using Cursor and v0. They don't write code; they describe outcomes, iterate on AI outputs, and deploy through engineering review gates. The economics work when features are isolated, iteration speed beats code elegance, and business value justifies technical debt. Simon Willison's analysis reveals this requires deliberate architecture: standalone services, API boundaries, AI-friendly tech stacks. It's not magic—it's careful boundaries.

- **Multi-model orchestration beats single-model loyalty.** Perplexity's Council Mode uses different models for different research subtasks—Opus 4.6 for complex reasoning, cheaper models for simple queries. The chairman model coordinates, specialized models execute. Microsoft's agent survey shows similar patterns: successful deployments orchestrate multiple agents with human-in-the-loop gates, not one autonomous system. The shift: from "which model?" to "how do we route tasks across models?" Economics matter—use expensive models only where needed.

- **Infrastructure for agent operations is shipping.** Vercel's Sandbox hit GA for secure code execution in production. Lenny's MCP walkthrough shows agents autonomously updating CRMs and preparing meeting briefs through standardized connectors. GitHub's agentic CI embeds agents into deployment pipelines—they generate tests, update docs, and pre-review PRs. Teresa's context rot analysis explains why products implement session boundaries and compression strategies. The pattern: agent success depends on operational infrastructure, not just model capability.

## Essential Resources

1. **Lenny Rachitsky** — [Getting Paid to Vibe Code: The New AI-Era Job](https://www.lennysnewsletter.com/p/getting-paid-to-vibe-code) — How StrongDM's 3-person non-technical team ships production features using AI tools.

2. **Simon Willison** — [How StrongDM's AI Team Builds Serious Software Without Coding](https://simonwillison.net/2026/Feb/7/software-factory/) — Technical analysis of architectural patterns enabling vibe coding to work at scale.

3. **Microsoft** — [Survey: 5 Practices for Successful AI Agent Deployments](https://www.microsoft.com/en-us/worklab/agents-are-here-is-your-company-prepared) — Enterprise research on organizational readiness patterns that distinguish working agent deployments from failures.

---

*9 daily updates tracked this month. [View all February updates](/?from=2026-02-01&to=2026-02-28)*
---

*9 daily updates tracked 9 items this month. [View all February updates](/?from=2026-02-01&to=2026-02-28)*