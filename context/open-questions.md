# Open Questions

Questions to investigate as relevant signals emerge.

This file captures unresolved product management questions, hypotheses, and bets related to AI products. It is a living context document that guides what the research assistant should pay extra attention to. These are NOT predictions. These are questions that active product leaders would want evidence on.

---

## How are user expectations for AI product quality and reliability evolving?

**Why this matters to product:**  
Product managers need to understand the quality bar users expect from AI features. If expectations are rising faster than capabilities, products will disappoint. If expectations are lower, there's opportunity to exceed them and differentiate.

**Signals to watch for:**
- User complaints or praise about AI feature reliability
- Product launches that emphasize reliability or accuracy
- Changes in how products position AI capabilities (overpromising vs. underpromising)
- User behavior shifts when AI features fail or succeed
- Support ticket patterns related to AI features
- Product removals or deprecations of AI features due to quality issues

**Related companies / people:**
- OpenAI, Anthropic, Google, Microsoft (foundation model providers)
- Perplexity, Character.AI, Midjourney (consumer AI products)

**Time horizon:**  
Medium-term

---

## What's the retention pattern for AI-powered features versus traditional features?

**Why this matters to product:**  
PMs need to know if AI features drive long-term engagement or if they're novelty-driven. This affects prioritization, resource allocation, and product strategy.

**Signals to watch for:**
- Retention metrics shared by companies with AI features
- Product announcements about AI feature adoption and retention
- Feature usage patterns over time (do users stick with AI features?)
- Product decisions to expand or reduce AI feature investment
- User feedback patterns on AI feature stickiness

**Related companies / people:**
- Notion, GitHub, Microsoft (products adding AI to existing workflows)
- Lenny Rachitsky, Andrew Chen (growth pattern analysis)

**Time horizon:**  
Medium-term

---

## Are users willing to pay for AI features, or do they expect them to be free?

**Why this matters to product:**  
Monetization strategy depends on willingness to pay. If users expect AI features free, products need different business models. If they'll pay, there's revenue opportunity.

**Signals to watch for:**
- Pricing changes for AI features (free to paid, paid to free)
- Subscription tier changes that include or exclude AI
- User reactions to AI feature pricing
- Product launches that position AI as premium vs. standard
- Revenue impact of AI feature monetization (if shared)

**Related companies / people:**
- OpenAI, Anthropic (API pricing changes)
- Notion, GitHub, Microsoft (subscription tier changes)
- Character.AI, Midjourney (consumer monetization)

**Time horizon:**  
Short-term to Medium-term

---

## How much autonomy are users comfortable giving to AI agents?

**Why this matters to product:**  
Agentic workflows require user trust. Too much autonomy might cause errors users can't recover from. Too little autonomy might not provide enough value. PMs need to find the right balance.

**Signals to watch for:**
- Product launches that increase or decrease agent autonomy
- User feedback on agent mistakes and recovery
- Product features that add or remove user control over agents
- Error rates and user tolerance patterns
- Product positioning around agent capabilities (autonomous vs. assisted)

**Related companies / people:**
- LangChain, Mastra (agent framework developers)
- OpenAI (Assistants API evolution)
- Products building agentic workflows

**Time horizon:**  
Medium-term

---

## What's the build vs. buy pattern for AI capabilities in products?

**Why this matters to product:**  
PMs face constant build vs. buy decisions. With AI, the tradeoffs are changing rapidly. Understanding when companies build vs. buy helps inform strategy.

**Signals to watch for:**
- Product announcements that switch from API to in-house models (or vice versa)
- Developer tooling that makes building easier
- Cost structure changes that affect build vs. buy economics
- Product capabilities that require custom vs. off-the-shelf solutions
- Company acquisitions of AI capabilities vs. building them

**Related companies / people:**
- Companies using OpenAI/Anthropic APIs vs. building their own
- Replicate, Hugging Face (making buying easier)
- LangChain, Mastra (making building easier)

**Time horizon:**  
Short-term to Medium-term

---

## How do AI features change product development velocity and iteration speed?

**Why this matters to product:**  
If AI accelerates development, PMs can ship faster. If it slows things down (due to quality concerns, testing complexity, etc.), that affects planning and resource allocation.

**Signals to watch for:**
- Product launch cadence changes after AI feature adoption
- Developer tooling that speeds up AI feature development
- Product iteration patterns (faster or slower with AI?)
- Team structure changes related to AI development
- Time-to-market for AI features vs. traditional features

**Related companies / people:**
- GitHub, Cursor (developer tools that affect velocity)
- Companies shipping AI features frequently

**Time horizon:**  
Medium-term

---

## What's the pattern for AI feature adoption curves compared to traditional features?

**Why this matters to product:**  
Understanding adoption patterns helps PMs set expectations, plan resources, and understand if AI features follow different adoption dynamics than traditional features.

**Signals to watch for:**
- Product announcements with adoption metrics
- User behavior data showing AI feature adoption rates
- Product decisions based on adoption patterns
- Comparison of AI feature adoption vs. non-AI feature adoption
- Viral or network effects in AI feature adoption

**Related companies / people:**
- Andrew Chen, Brian Balfour (growth pattern analysis)
- Products with public adoption metrics

**Time horizon:**  
Short-term to Medium-term

---

## How do AI products handle trust, error recovery, and user control?

**Why this matters to product:**  
AI makes mistakes. How products handle errors, allow user control, and build trust affects adoption and retention. PMs need patterns for error handling and trust-building.

**Signals to watch for:**
- Product features that add user control or override capabilities
- Error handling and recovery patterns
- Product positioning around reliability and trust
- User feedback on trust and control issues
- Product changes that increase or decrease transparency

**Related companies / people:**
- Products with high-stakes AI features (fintech, healthcare)
- Products emphasizing trust and reliability

**Time horizon:**  
Short-term to Medium-term

---

## What's the impact of AI on product differentiation and competitive moats?

**Why this matters to product:**  
If AI capabilities are easily replicable, products can't differentiate on AI alone. PMs need to understand what creates sustainable differentiation in AI products.

**Signals to watch for:**
- Product positioning that emphasizes unique AI capabilities
- Competitive responses to AI feature launches
- Product differentiation strategies beyond AI
- Market consolidation or fragmentation patterns
- Product moats that survive AI commoditization

**Related companies / people:**
- Products competing in similar AI spaces
- Sarah Guo, Marc Andreessen (market dynamics analysis)

**Time horizon:**  
Long-term

---

## How are PM roles, responsibilities, and team structures changing with AI?

**Why this matters to product:**  
PMs need to adapt their craft. Understanding how roles are changing helps PMs prepare and helps organizations structure teams effectively.

**Signals to watch for:**
- Job descriptions and role changes for PMs in AI companies
- Team structure changes (PM + AI engineer, etc.)
- PM skill requirements in AI product roles
- Product process changes (discovery, prioritization, etc.)
- Org design patterns in AI product teams

**Related people:**
- Marty Cagan, Teresa Torres, Melissa Perri (PM craft experts)
- Lara Hogan (org design)

**Time horizon:**  
Medium-term to Long-term

---

## What's the pattern for AI feature integration into existing products versus standalone AI products?

**Why this matters to product:**  
PMs need to decide whether to add AI to existing products or build new AI-native products. Understanding what works helps inform strategy.

**Signals to watch for:**
- Product launches that add AI to existing workflows
- Standalone AI product launches
- User adoption patterns for integrated vs. standalone
- Product strategy shifts (integrated to standalone or vice versa)
- Success patterns for each approach

**Related companies / people:**
- Notion, GitHub, Microsoft (integration approach)
- Perplexity, Character.AI (standalone approach)

**Time horizon:**  
Medium-term

---

## How do AI products balance capability improvements with cost and latency?

**Why this matters to product:**  
Better AI capabilities often come with higher costs or slower responses. PMs need to understand tradeoffs and how products balance these constraints.

**Signals to watch for:**
- Product decisions that prioritize cost vs. capability
- Latency improvements or degradations
- Pricing changes related to model upgrades
- Product positioning around speed vs. quality
- User tolerance for latency in different contexts

**Related companies / people:**
- OpenAI, Anthropic (model capability vs. cost tradeoffs)
- Products optimizing for latency (search, real-time features)

**Time horizon:**  
Short-term to Medium-term

---
